{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lista_5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMc_ou9GHQCL"
      },
      "source": [
        "# **LISTA 5**\n",
        "\n",
        "**Edvonaldo Horácio e José Carlos Pinheiro**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCZslcHqH2tv"
      },
      "source": [
        "1. O objetivo dessa questão  ́e desenvolver um buscador de documentos.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFJqiX16k79C"
      },
      "source": [
        "folder_path = '/content/tech'\n",
        "#folder_path = '/content/drive/My Drive/tech'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWRK4awOM-sP",
        "outputId": "9d453a3f-f962-4620-e4d5-ba7218c7f628",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dI1wSLnGsf1Q"
      },
      "source": [
        "import os\n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import glob\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tyf32MV96-F_"
      },
      "source": [
        "# **Pré processamento**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrIx1xS_k9_2",
        "outputId": "b3f20c83-3648-48e2-94cf-ca5189440f89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "## Pré processamento\n",
        "\n",
        "texts = []\n",
        "\n",
        "\n",
        "for filename in glob.glob(os.path.join(folder_path, '*.txt')):\n",
        "\twith open(filename, 'r') as f_in:\n",
        "\t\ttexts.append([filename.split('/')[-1].split('.')[0], f_in.read()])\n",
        "\n",
        "df = pd.DataFrame.from_records(texts)\n",
        "df.columns = [\"id\", \"content\"]\n",
        "print(df.tail())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-100-bdf668939052>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   5285\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5286\u001b[0m             \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5287\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5288\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5289\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/properties.pyx\u001b[0m in \u001b[0;36mpandas._libs.properties.AxisProperty.__set__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_set_axis\u001b[0;34m(self, axis, labels)\u001b[0m\n\u001b[1;32m    659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mset_axis\u001b[0;34m(self, axis, new_labels)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_len\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mold_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             raise ValueError(\n\u001b[0;32m--> 178\u001b[0;31m                 \u001b[0;34mf\"Length mismatch: Expected axis has {old_len} elements, new \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m                 \u001b[0;34mf\"values have {new_len} elements\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: Length mismatch: Expected axis has 0 elements, new values have 2 elements"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TSS5t5OstxQ"
      },
      "source": [
        "## Removendo palavras com números\n",
        "df = df.replace(to_replace=r'\\w*\\d\\w*', value='', regex=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wZwoMrqsx75",
        "outputId": "6bdc8525-c025-4b66-ca54-1ec3a23f60bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "## Removendo pontuação\n",
        "\n",
        "import re\n",
        "import string\n",
        "def remove_punctuation(text):\n",
        "  return  re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
        "'''\n",
        "df['content'] = df['content'].apply(remove_punctuation) # Removendo pontuação\n",
        "'''\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\ndf['content'] = df['content'].apply(remove_punctuation) # Removendo pontuação\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbNxE2GCs7mj"
      },
      "source": [
        "# Converter todas as palavras em minúsculas\n",
        "for i in range(df['content'].shape[0]):\n",
        "  df['content'][i] = df['content'][i].lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYL53CesyvN4",
        "outputId": "cb86dea4-f78e-4419-d84f-5f5844ccce33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "df['content']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      satellite mapping aids darfur relief\\n\\naid wo...\n",
              "1      bond game fails to shake or stir\\n\\nfor gaming...\n",
              "2      gamer buys $, virtual land\\n\\na -year-old game...\n",
              "3      latest opera browser gets vocal\\n\\nnet browser...\n",
              "4      peer-to-peer nets 'here to stay'\\n\\npeer-to-pe...\n",
              "                             ...                        \n",
              "396    eu software patent law delayed\\n\\ncontroversia...\n",
              "397    musicians 'upbeat' about the net\\n\\nmusicians ...\n",
              "398    'blog' picked as word of the year\\n\\nthe term ...\n",
              "399    napster offers rented music to go\\n\\nmusic dow...\n",
              "400    broadband in the uk gathers pace\\n\\none person...\n",
              "Name: content, Length: 401, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3BfxliY0YZI",
        "outputId": "c3ae7a21-dff1-4425-8170-939f31c8ad31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "for i in range(df['content'].shape[0]):\n",
        "  df['content'][i] = df['content'][i].replace(\"\\n\", \" \")\n",
        "\n",
        "df['content']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      apple unveils low-cost 'mac mini'  apple has u...\n",
              "1      commodore finds new lease of life  the once-fa...\n",
              "2      sony psp tipped as a 'must-have'  sony's plays...\n",
              "3      ink helps drive democracy in asia  the kyrgyz ...\n",
              "4      ask jeeves joins web log market  ask jeeves ha...\n",
              "                             ...                        \n",
              "396    multi-purpose tv aids india  two-thirds of the...\n",
              "397    us state acts to stop 'spammers'  us state tex...\n",
              "398    new delay hits eu software laws  a fresh delay...\n",
              "399    hacker threat to apple's itunes  users of appl...\n",
              "400    apple mac mini gets warm welcome  the mac mini...\n",
              "Name: content, Length: 401, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgFN7thvyRY7",
        "outputId": "f6043adf-990f-4353-9bce-332faebf6009",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize \n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "data = []\n",
        "for text in range(df['content'].shape[0]):\n",
        "  data_aux = []\n",
        "  for i in sent_tokenize(df['content'][text]): \n",
        "      temp = [] \n",
        "\n",
        "      # tokenize sentença para palavra \n",
        "      for j in word_tokenize(remove_punctuation(i)): ## Retirando a pontuação\n",
        "          \n",
        "          temp.append(j.lower()) \n",
        "\n",
        "      data_aux.append(temp) \n",
        "  data.append((text, data_aux))   \n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoKiGMeY4CTE",
        "outputId": "a441a618-aa67-4429-b9f9-8d4dd61d2cb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(len(data))\n",
        "data[0][1][0][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "401\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'apple'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFDzxkBb7WVp"
      },
      "source": [
        "# **Treinamento**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MHyBT8167OS",
        "outputId": "79c32017-6ed6-4c21-8984-e54556bb4016",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "import gensim \n",
        "from gensim.test.utils import common_texts\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "model = Word2Vec(common_texts, size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "common_texts"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['human', 'interface', 'computer'],\n",
              " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
              " ['eps', 'user', 'interface', 'system'],\n",
              " ['system', 'human', 'system', 'eps'],\n",
              " ['user', 'response', 'time'],\n",
              " ['trees'],\n",
              " ['graph', 'trees'],\n",
              " ['graph', 'minors', 'trees'],\n",
              " ['graph', 'minors', 'survey']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9J3fOA1UA-t9",
        "outputId": "6ff528ed-4e7a-4b46-a183-d4d9a8f65933",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import gensim.downloader as api\n",
        "word_vectors50 = api.load(\"glove-wiki-gigaword-50\")\n",
        "word_vectors100 = api.load(\"glove-wiki-gigaword-100\")\n",
        "word_vectors200 = api.load(\"glove-wiki-gigaword-200\")\n",
        "\n",
        "models = [(word_vectors50, 'Com 50'), (word_vectors100, 'Com 100'), (word_vectors200, 'Com 200')]\n",
        "\n",
        "#models = [(word_vectors50, 'Com 50')]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n",
            "[==================================================] 100.0% 252.1/252.1MB downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1bwzkR-nsUg"
      },
      "source": [
        "# Primeira questão"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1L-RiAuHnkfa",
        "outputId": "a1816321-7505-4af9-cfb6-b6361151a57e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "words_to_compare = ['walmart', 'pineapple', 'platypus', 'python', 'javascript']\n",
        "for model in models:\n",
        "  print('***************** Resultados ' + model[1] + ' *****************')\n",
        "  for word in words_to_compare:\n",
        "    result = []\n",
        "    for docum in range(0, len(data)): ##Por documento\n",
        "      for vects in range(0, len(data[docum][1])): ## Vetor de vetores de palavras\n",
        "        for arr in range(0, len(data[docum][1][vects])): ## Vetor de palavra\n",
        "          try:\n",
        "            word_acc = data[docum][1][vects][arr] ## palavra\n",
        "            similarity = model[0].similarity(word, word_acc)\n",
        "            result.append((data[docum][1][vects][arr], docum, similarity))\n",
        "          except Exception as e:\n",
        "            pass\n",
        "          finally:\n",
        "            pass\n",
        "    result.sort(key=lambda tup: tup[2], reverse=True)\n",
        "    print('Para a palavra ' + word)\n",
        "    print(\"1° {palavra} | texto: {text} | similaridade de {sim:.2f}\".format(palavra = result[0][0], text = result[0][1], sim = result[0][2]) )\n",
        "    id = 1\n",
        "    while(result[0][0] == result[id][0]):\n",
        "      id +=1\n",
        "    print(\"2° {palavra} | texto: {text} | similaridade de {sim:.2f}\".format(palavra = result[id][0], text = result[id][1], sim = result[id][2]) )\n",
        "    id2 = id + 1\n",
        "    while(result[id2][0] == result[id][0]):\n",
        "      id2 +=1\n",
        "    print(\"3° {palavra} | texto: {text} | similaridade de {sim:.2f}\".format(palavra = result[id2][0], text = result[id2][1], sim = result[id2][2]) )\n",
        "    print(\"----------------------------------------------------------------\")\n",
        "\n",
        "    ## Médio\n",
        "  for word in words_to_compare:\n",
        "    result = []\n",
        "    for docum in range(0, len(data)): ##Por documento\n",
        "      for vects in range(0, len(data[docum][1])): ## Vetor de vetores de palavras\n",
        "        diff = 0\n",
        "        for arr in range(0, len(data[docum][1][vects])): ## Vetor de palavra\n",
        "          try:\n",
        "            word_acc = data[docum][1][vects][arr] ## palavra\n",
        "            diff += model[0].similarity(word, word_acc)\n",
        "          except Exception as e:\n",
        "            pass\n",
        "          finally:\n",
        "            pass\n",
        "        if(len(data[docum][1][vects]) > 0):\n",
        "          result.append((data[docum][1][vects], docum, (diff/len(data[docum][1][vects]))))\n",
        "    result.sort(key=lambda tup: tup[2], reverse=True)\n",
        "    print('Para a palavra ' + word + ' Vector Medios')\n",
        "    print(\"1° {palavra} | texto: {text} | similaridade de {sim:.2f}\".format(palavra = result[0][0], text = result[0][1], sim = result[0][2]) )\n",
        "    print(\"2° {palavra} | texto: {text} | similaridade de {sim:.2f}\".format(palavra = result[1][0], text = result[1][1], sim = result[1][2]) )\n",
        "    print(\"3° {palavra} | texto: {text} | similaridade de {sim:.2f}\".format(palavra = result[2][0], text = result[2][1], sim = result[2][2]) )\n",
        "    print(\"----------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***************** Resultados Com 50 *****************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Para a palavra walmart\n",
            "1° retailer | texto: 295 | similaridade de 0.72\n",
            "2° stores | texto: 18 | similaridade de 0.69\n",
            "3° tesco | texto: 50 | similaridade de 0.68\n",
            "----------------------------------------------------------------\n",
            "Para a palavra pineapple\n",
            "1° juice | texto: 42 | similaridade de 0.81\n",
            "2° plum | texto: 232 | similaridade de 0.78\n",
            "3° honey | texto: 30 | similaridade de 0.72\n",
            "----------------------------------------------------------------\n",
            "Para a palavra platypus\n",
            "1° hamster | texto: 46 | similaridade de 0.62\n",
            "2° feline | texto: 125 | similaridade de 0.57\n",
            "3° qrio | texto: 25 | similaridade de 0.57\n",
            "----------------------------------------------------------------\n",
            "Para a palavra python\n",
            "1° mouse | texto: 0 | similaridade de 0.63\n",
            "2° scripting | texto: 149 | similaridade de 0.60\n",
            "3° ape | texto: 174 | similaridade de 0.59\n",
            "----------------------------------------------------------------\n",
            "Para a palavra javascript\n",
            "1° scripting | texto: 149 | similaridade de 0.80\n",
            "2° xml | texto: 153 | similaridade de 0.72\n",
            "3° interfaces | texto: 97 | similaridade de 0.72\n",
            "----------------------------------------------------------------\n",
            "Para a palavra walmart Vector Medios\n",
            "1° ['million', 'broadband', 'subscribers'] | texto: 382 | similaridade de 0.31\n",
            "2° ['million', 'customers'] | texto: 295 | similaridade de 0.25\n",
            "3° ['million', 'units', 'worldwide'] | texto: 177 | similaridade de 0.22\n",
            "----------------------------------------------------------------\n",
            "Para a palavra pineapple Vector Medios\n",
            "1° ['lol'] | texto: 199 | similaridade de 0.31\n",
            "2° ['what', 'about', 'the', 'soda', 'stream'] | texto: 162 | similaridade de 0.23\n",
            "3° ['the', 'mac', 'mini', 'is', 'just', 'a', 'box'] | texto: 321 | similaridade de 0.22\n",
            "----------------------------------------------------------------\n",
            "Para a palavra platypus Vector Medios\n",
            "1° ['teraflops'] | texto: 36 | similaridade de 0.20\n",
            "2° ['teraflops'] | texto: 206 | similaridade de 0.20\n",
            "3° ['lol'] | texto: 199 | similaridade de 0.06\n",
            "----------------------------------------------------------------\n",
            "Para a palavra python Vector Medios\n",
            "1° ['ghz'] | texto: 325 | similaridade de 0.40\n",
            "2° ['a', 'monitor', 'keyboard', 'or', 'mouse'] | texto: 321 | similaridade de 0.37\n",
            "3° ['a'] | texto: 321 | similaridade de 0.33\n",
            "----------------------------------------------------------------\n",
            "Para a palavra javascript Vector Medios\n",
            "1° ['mozilla', 'browsers', 'including', 'firefox'] | texto: 156 | similaridade de 0.43\n",
            "2° ['ghz'] | texto: 325 | similaridade de 0.37\n",
            "3° ['the', 'software', 'uses', 'content', 'recognition', 'algorithms'] | texto: 121 | similaridade de 0.36\n",
            "----------------------------------------------------------------\n",
            "***************** Resultados Com 100 *****************\n",
            "Para a palavra walmart\n",
            "1° stores | texto: 18 | similaridade de 0.59\n",
            "2° tesco | texto: 50 | similaridade de 0.57\n",
            "3° retailer | texto: 295 | similaridade de 0.57\n",
            "----------------------------------------------------------------\n",
            "Para a palavra pineapple\n",
            "1° juice | texto: 42 | similaridade de 0.69\n",
            "2° potato | texto: 162 | similaridade de 0.58\n",
            "3° plum | texto: 232 | similaridade de 0.58\n",
            "----------------------------------------------------------------\n",
            "Para a palavra platypus\n",
            "1° feline | texto: 125 | similaridade de 0.45\n",
            "2° ape | texto: 174 | similaridade de 0.41\n",
            "3° starship | texto: 75 | similaridade de 0.40\n",
            "----------------------------------------------------------------\n",
            "Para a palavra python\n",
            "1° scripting | texto: 149 | similaridade de 0.49\n",
            "2° spider | texto: 47 | similaridade de 0.47\n",
            "3° ninja | texto: 236 | similaridade de 0.46\n",
            "----------------------------------------------------------------\n",
            "Para a palavra javascript\n",
            "1° scripting | texto: 149 | similaridade de 0.72\n",
            "2° xml | texto: 153 | similaridade de 0.62\n",
            "3° graphical | texto: 5 | similaridade de 0.59\n",
            "----------------------------------------------------------------\n",
            "Para a palavra walmart Vector Medios\n",
            "1° ['sony', 'net', 'minidisc'] | texto: 162 | similaridade de 0.20\n",
            "2° ['sony', 'net', 'minidisc'] | texto: 266 | similaridade de 0.20\n",
            "3° ['million', 'customers'] | texto: 295 | similaridade de 0.16\n",
            "----------------------------------------------------------------\n",
            "Para a palavra pineapple Vector Medios\n",
            "1° ['lol'] | texto: 199 | similaridade de 0.19\n",
            "2° ['or', 'an', 'ipod'] | texto: 162 | similaridade de 0.17\n",
            "3° ['teraflops'] | texto: 36 | similaridade de 0.15\n",
            "----------------------------------------------------------------\n",
            "Para a palavra platypus Vector Medios\n",
            "1° ['lol'] | texto: 199 | similaridade de 0.24\n",
            "2° ['teraflops'] | texto: 36 | similaridade de 0.06\n",
            "3° ['teraflops'] | texto: 206 | similaridade de 0.06\n",
            "----------------------------------------------------------------\n",
            "Para a palavra python Vector Medios\n",
            "1° ['ghz'] | texto: 325 | similaridade de 0.24\n",
            "2° ['a', 'monitor', 'keyboard', 'or', 'mouse'] | texto: 321 | similaridade de 0.22\n",
            "3° ['or', 'a', 'pocket', 'calculator'] | texto: 162 | similaridade de 0.20\n",
            "----------------------------------------------------------------\n",
            "Para a palavra javascript Vector Medios\n",
            "1° ['mozilla', 'browsers', 'including', 'firefox'] | texto: 156 | similaridade de 0.38\n",
            "2° ['the', 'software', 'uses', 'content', 'recognition', 'algorithms'] | texto: 121 | similaridade de 0.25\n",
            "3° ['ghz'] | texto: 325 | similaridade de 0.24\n",
            "----------------------------------------------------------------\n",
            "***************** Resultados Com 200 *****************\n",
            "Para a palavra walmart\n",
            "1° stores | texto: 18 | similaridade de 0.53\n",
            "2° retailer | texto: 295 | similaridade de 0.49\n",
            "3° tesco | texto: 50 | similaridade de 0.46\n",
            "----------------------------------------------------------------\n",
            "Para a palavra pineapple\n",
            "1° juice | texto: 42 | similaridade de 0.63\n",
            "2° fruit | texto: 195 | similaridade de 0.52\n",
            "3° sliced | texto: 76 | similaridade de 0.52\n",
            "----------------------------------------------------------------\n",
            "Para a palavra platypus\n",
            "1° gatekeeper | texto: 171 | similaridade de 0.33\n",
            "2° humanoid | texto: 25 | similaridade de 0.32\n",
            "3° dinosaurs | texto: 60 | similaridade de 0.30\n",
            "----------------------------------------------------------------\n",
            "Para a palavra python\n",
            "1° scripting | texto: 149 | similaridade de 0.49\n",
            "2° grail | texto: 161 | similaridade de 0.45\n",
            "3° parrot | texto: 163 | similaridade de 0.42\n",
            "----------------------------------------------------------------\n",
            "Para a palavra javascript\n",
            "1° scripting | texto: 149 | similaridade de 0.65\n",
            "2° xml | texto: 153 | similaridade de 0.58\n",
            "3° functionality | texto: 11 | similaridade de 0.53\n",
            "----------------------------------------------------------------\n",
            "Para a palavra walmart Vector Medios\n",
            "1° ['sony', 'net', 'minidisc'] | texto: 162 | similaridade de 0.20\n",
            "2° ['sony', 'net', 'minidisc'] | texto: 266 | similaridade de 0.20\n",
            "3° ['million', 'customers'] | texto: 295 | similaridade de 0.14\n",
            "----------------------------------------------------------------\n",
            "Para a palavra pineapple Vector Medios\n",
            "1° ['teraflops'] | texto: 36 | similaridade de 0.19\n",
            "2° ['teraflops'] | texto: 206 | similaridade de 0.19\n",
            "3° ['lol'] | texto: 199 | similaridade de 0.13\n",
            "----------------------------------------------------------------\n",
            "Para a palavra platypus Vector Medios\n",
            "1° ['lol'] | texto: 199 | similaridade de 0.26\n",
            "2° ['teraflops'] | texto: 36 | similaridade de 0.09\n",
            "3° ['teraflops'] | texto: 206 | similaridade de 0.09\n",
            "----------------------------------------------------------------\n",
            "Para a palavra python Vector Medios\n",
            "1° ['mozilla', 'browsers', 'including', 'firefox'] | texto: 156 | similaridade de 0.23\n",
            "2° ['ghz'] | texto: 325 | similaridade de 0.17\n",
            "3° ['the', 'release', 'of', 'firefox'] | texto: 178 | similaridade de 0.15\n",
            "----------------------------------------------------------------\n",
            "Para a palavra javascript Vector Medios\n",
            "1° ['mozilla', 'browsers', 'including', 'firefox'] | texto: 156 | similaridade de 0.38\n",
            "2° ['ghz'] | texto: 325 | similaridade de 0.27\n",
            "3° ['lol'] | texto: 199 | similaridade de 0.19\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEHPQPE0qlDZ"
      },
      "source": [
        "# Segunda questão"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rY2ED8Y2kjuv"
      },
      "source": [
        "# Problema: quais textos são sobre segurança tecnológica? (yes - no)\n",
        "# Rotulando os dados\n",
        "import os\n",
        "import glob\n",
        "import nltk\n",
        "import random\n",
        "import pandas as pd\n",
        "import random\n",
        "import re\n",
        "import string\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "path1_yes = '/content/yes_sec' # 47 arquivos\n",
        "path2_no = '/content/no_sec' # 53 arquivos\n",
        "\n",
        "txt_corpus = []\n",
        "\n",
        "# Lendo pasta 1\n",
        "for filename in glob.glob(os.path.join(path1_yes, '*.txt')):\n",
        "\twith open(filename, 'r') as f_in:\n",
        "\t\ttxt_corpus.append([filename.split('/')[-1].split('.')[0], f_in.read()])\n",
        "\n",
        "df_corpus = pd.DataFrame.from_records(txt_corpus)\n",
        "df_corpus.columns = [\"id\", \"content\"]\n",
        "\n",
        "# Lendo pasta 2\n",
        "for filename in glob.glob(os.path.join(path2_no, '*.txt')):\n",
        "\twith open(filename, 'r') as f_in:\n",
        "\t\ttxt_corpus.append([filename.split('/')[-1].split('.')[0], f_in.read()])\n",
        "\n",
        "df_corpus = pd.DataFrame.from_records(txt_corpus)\n",
        "df_corpus.columns = [\"id\", \"content\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5SsNb5xmXhl"
      },
      "source": [
        "#from sklearn.utils import shuffle\n",
        "#df_merge = shuffle(df_merge)\n",
        "#df_merge.head(5)\n",
        "df_corpus.insert(loc=0,column=\"label\",value=\"\")\n",
        "for i in range(0,47):\n",
        "  df_corpus['label'][i] = \"1\" # Yes\n",
        "\n",
        "for i in range(47,100):\n",
        "  df_corpus['label'][i] = \"0\" # No"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GlVLtwsmnwr",
        "outputId": "2aa36580-4247-49fd-e711-015184590770",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# STEP 1 - PREPARANDO OS DADOS\n",
        "# Removendo as palavras que contém numeros\n",
        "df_corpus['content'] = df_corpus['content'].replace(to_replace=r'\\w*\\d\\w*', value='', regex=True)\n",
        "df_corpus.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>id</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>055</td>\n",
              "      <td>Junk e-mails on relentless rise\\n\\nSpam traffi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>022</td>\n",
              "      <td>Sun offers processing by the hour\\n\\nSun Micro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>188</td>\n",
              "      <td>Call for action on internet scam\\n\\nPhone comp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>031</td>\n",
              "      <td>Solutions to net security fears\\n\\nFake bank e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>136</td>\n",
              "      <td>BT offers free net phone calls\\n\\nBT is offeri...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label   id                                            content\n",
              "0     1  055  Junk e-mails on relentless rise\\n\\nSpam traffi...\n",
              "1     1  022  Sun offers processing by the hour\\n\\nSun Micro...\n",
              "2     1  188  Call for action on internet scam\\n\\nPhone comp...\n",
              "3     1  031  Solutions to net security fears\\n\\nFake bank e...\n",
              "4     1  136  BT offers free net phone calls\\n\\nBT is offeri..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPnQTvUjm6-u",
        "outputId": "f9cdafc2-1e8d-4989-8aee-367a56844bb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# Removendo pontuação e colocando tudo em minúsculo\n",
        "def remove_punctuation(text):\n",
        "  return  re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
        "\n",
        "df_corpus['content'] = df_corpus['content'].apply(remove_punctuation) # Removendo pontuação\n",
        "\n",
        "df_corpus.content = df_corpus.content.str.lower()\n",
        "df_corpus.content"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     junk e mails on relentless rise\\n\\nspam traffi...\n",
              "1     sun offers processing by the hour\\n\\nsun micro...\n",
              "2     call for action on internet scam\\n\\nphone comp...\n",
              "3     solutions to net security fears\\n\\nfake bank e...\n",
              "4     bt offers free net phone calls\\n\\nbt is offeri...\n",
              "                            ...                        \n",
              "95    commodore finds new lease of life\\n\\nthe once ...\n",
              "96    progress on new internet domains\\n\\nby early  ...\n",
              "97    when invention turns to innovation\\n\\nit is un...\n",
              "98    pompeii gets digital make over\\n\\nthe old fash...\n",
              "99    broadband in the uk growing fast\\n\\nhigh speed...\n",
              "Name: content, Length: 100, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1OJUedzoklx",
        "outputId": "3a833292-9b0c-4d33-bd80-1be88a54d3b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# Embaralhando os dados\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "df_corpus = shuffle(df_corpus)\n",
        "df_corpus.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>id</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>0</td>\n",
              "      <td>014</td>\n",
              "      <td>eu software patent law faces axe\\n\\nthe europe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>1</td>\n",
              "      <td>164</td>\n",
              "      <td>who do you think you are \\n\\nthe real danger i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>0</td>\n",
              "      <td>112</td>\n",
              "      <td>portable playstation ready to go\\n\\nsony s pla...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>0</td>\n",
              "      <td>001</td>\n",
              "      <td>ink helps drive democracy in asia\\n\\nthe kyrgy...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>0</td>\n",
              "      <td>025</td>\n",
              "      <td>sony psp console hits us in march\\n\\nus gamers...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label   id                                            content\n",
              "85     0  014  eu software patent law faces axe\\n\\nthe europe...\n",
              "34     1  164  who do you think you are \\n\\nthe real danger i...\n",
              "88     0  112  portable playstation ready to go\\n\\nsony s pla...\n",
              "67     0  001  ink helps drive democracy in asia\\n\\nthe kyrgy...\n",
              "68     0  025  sony psp console hits us in march\\n\\nus gamers..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5SavOWZYtkM",
        "outputId": "591313bb-db63-48b8-83d2-a152aec6e730",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "df_corpus_noid = df_corpus\n",
        "del df_corpus_noid['id']\n",
        "print(df_corpus_noid.head(5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   label                                            content\n",
            "85     0  eu software patent law faces axe\\n\\nthe europe...\n",
            "34     1  who do you think you are \\n\\nthe real danger i...\n",
            "88     0  portable playstation ready to go\\n\\nsony s pla...\n",
            "67     0  ink helps drive democracy in asia\\n\\nthe kyrgy...\n",
            "68     0  sony psp console hits us in march\\n\\nus gamers...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfYPgVAuoqmK",
        "outputId": "5058b735-b2c8-43a2-dbd7-a499ffb1b391",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# STEP 2 - SPLIT NOS DADOS\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#x = df_corpus.content\n",
        "#y = df_corpus.label\n",
        "\n",
        "#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
        "train, test = train_test_split(df_corpus_noid, test_size=0.33, random_state=42)\n",
        "\n",
        "print(\"Train: \",train.head())\n",
        "print(\"Test: \", test.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train:     label                                            content\n",
            "47     0  china net cafe culture crackdown\\n\\nchinese au...\n",
            "40     1   evil twin  fear for wireless net\\n\\npeople us...\n",
            "55     0  gadget show heralds  season\\n\\npartners of tho...\n",
            "97     0  when invention turns to innovation\\n\\nit is un...\n",
            "10     1  screensaver tackles spam websites\\n\\nnet users...\n",
            "Test:     label                                            content\n",
            "83     0  global blogger action day called\\n\\nthe global...\n",
            "28     1  concern over rfid tags\\n\\nconsumers are very c...\n",
            "43     1  technology gets the creative bug\\n\\nthe hi tec...\n",
            "69     0  blogger grounded by her airline\\n\\na us airlin...\n",
            "64     0  reboot ordered for eu patent law\\n\\na european...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJuzUvsazB6o",
        "outputId": "2dec1709-db1d-47f0-857a-c087f6bb36ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from gensim.test.utils import common_texts\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import utils\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "import multiprocessing\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "def tokenize_text(text):\n",
        "    tokens = []\n",
        "    for sent in nltk.sent_tokenize(text):\n",
        "        for word in nltk.word_tokenize(sent):\n",
        "            if len(word) < 2:\n",
        "                continue\n",
        "            tokens.append(word.lower())\n",
        "    return tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gm7Pz3F9zRTF",
        "outputId": "f1807cab-a481-407f-e584-fcfb5687ff72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#df12 = df['content'].apply(tokenize_text)\n",
        "#df12.tolist()\n",
        "train_tagged = train.apply(lambda r: TaggedDocument(words=tokenize_text(r['content']), tags=[r.label]), axis=1)\n",
        "test_tagged = test.apply(lambda r: TaggedDocument(words=tokenize_text(r['content']), tags=[r.label]), axis=1)\n",
        "\n",
        "train_tagged.values[30]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TaggedDocument(words=['broadband', 'in', 'the', 'uk', 'growing', 'fast', 'high', 'speed', 'net', 'connections', 'in', 'the', 'uk', 'are', 'proving', 'more', 'popular', 'than', 'ever', 'bt', 'reports', 'that', 'more', 'people', 'signed', 'up', 'for', 'broadband', 'in', 'the', 'last', 'three', 'months', 'than', 'in', 'any', 'other', 'quarter', 'the', 'connections', 'take', 'the', 'total', 'number', 'of', 'people', 'in', 'the', 'uk', 'signing', 'up', 'for', 'broadband', 'from', 'bt', 'to', 'almost', 'million', 'nationally', 'more', 'than', 'million', 'browse', 'the', 'net', 'via', 'broadband', 'britain', 'now', 'has', 'among', 'the', 'highest', 'number', 'of', 'broadband', 'connections', 'throughout', 'the', 'whole', 'of', 'europe', 'according', 'to', 'figures', 'gathered', 'by', 'industry', 'watchdog', 'ofcom', 'the', 'growth', 'means', 'that', 'the', 'uk', 'has', 'now', 'surpassed', 'germany', 'in', 'terms', 'of', 'broadband', 'users', 'per', 'people', 'the', 'uk', 'total', 'of', 'million', 'translates', 'into', 'connections', 'per', 'people', 'compared', 'to', 'in', 'germany', 'and', 'in', 'the', 'netherlands', 'the', 'numbers', 'of', 'people', 'signing', 'up', 'to', 'broadband', 'include', 'those', 'that', 'get', 'their', 'service', 'direct', 'from', 'bt', 'or', 'via', 'the', 'many', 'companies', 'that', 're', 'sell', 'bt', 'lines', 'under', 'their', 'own', 'name', 'part', 'of', 'the', 'surge', 'in', 'people', 'signing', 'up', 'was', 'due', 'to', 'bt', 'stretching', 'the', 'reach', 'of', 'adsl', 'the', 'uk', 'most', 'widely', 'used', 'way', 'of', 'getting', 'broadband', 'beyond', 'asymmetric', 'digital', 'subscriber', 'line', 'technology', 'lets', 'ordinary', 'copper', 'phone', 'lines', 'support', 'high', 'data', 'speeds', 'the', 'standard', 'speed', 'is', 'though', 'faster', 'connections', 'are', 'available', 'this', 'breakthrough', 'led', 'to', 'dramatic', 'increase', 'in', 'orders', 'as', 'we', 'were', 'suddenly', 'able', 'to', 'satisfy', 'the', 'pent', 'up', 'demand', 'that', 'existed', 'in', 'many', 'areas', 'said', 'paul', 'reynolds', 'chief', 'executive', 'of', 'bt', 'wholesale', 'which', 'provides', 'phone', 'lines', 'that', 'other', 'firms', 're', 'sell', 'bt', 'retail', 'which', 'sells', 'net', 'services', 'under', 'its', 'own', 'name', 'also', 'had', 'good', 'quarter', 'and', 'provided', 'about', 'of', 'the', 'new', 'broadband', 'customers', 'this', 'was', 'slight', 'increase', 'on', 'the', 'previous', 'three', 'months', 'despite', 'the', 'good', 'news', 'about', 'growth', 'in', 'broadband', 'figures', 'from', 'telecommunications', 'regulator', 'ofcom', 'show', 'that', 'bt', 'faces', 'increasing', 'competition', 'and', 'dwindling', 'influence', 'in', 'other', 'sectors', 'local', 'loop', 'unbundling', 'llu', 'in', 'which', 'bt', 'rivals', 'install', 'their', 'hardware', 'in', 'exchanges', 'and', 'take', 'over', 'the', 'line', 'to', 'customer', 'home', 'or', 'office', 'is', 'growing', 'steadily', 'cable', 'wireless', 'and', 'ntl', 'have', 'announced', 'that', 'they', 'are', 'investing', 'millions', 'to', 'start', 'offering', 'llu', 'services', 'by', 'the', 'end', 'of', 'september', 'more', 'than', 'million', 'phone', 'lines', 'were', 'using', 'so', 'called', 'carrier', 'pre', 'section', 'cps', 'services', 'such', 'as', 'talktalk', 'and', 'one', 'tel', 'which', 'route', 'phone', 'calls', 'across', 'non', 'bt', 'networks', 'from', 'local', 'exchange', 'there', 'are', 'now', 'more', 'than', 'different', 'firms', 'offering', 'cps', 'services', 'and', 'the', 'percentage', 'of', 'people', 'using', 'bt', 'lines', 'for', 'voice', 'calls', 'has', 'shrunk', 'to'], tags=['0'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VVa9MHzziCg",
        "outputId": "18c7499e-fa83-42cb-cf8f-228f3c77893e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#tagged_data = [TaggedDocument(words=df12, tags=[str(i)]) for i, _d in enumerate(df12)]\n",
        "#print(tagged_data[1])\n",
        "# Construindo o vocabulário - Há o DBOW e o DM:\n",
        "# Usando o DBOW\n",
        "import multiprocessing\n",
        "\n",
        "my_cores = multiprocessing.cpu_count()\n",
        "\n",
        "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=my_cores)\n",
        "model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 67/67 [00:00<00:00, 183912.54it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_-1w2Kt7fZv",
        "outputId": "fd9c4c65-d615-40f4-8c40-93f7c657f59f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "#cores = multiprocessing.cpu_count()\n",
        "#model_dbow = Doc2Vec(tagged_data, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores, alpha=0.025, min_alpha=0.001)\n",
        "#model_dbow.build_vocab([x for x in tqdm(tagged_data)])\n",
        "# Treinando o Doc2Vec - DBOW\n",
        "import time\n",
        "\n",
        "# Tempo de execução\n",
        "def seconds_transform(seconds_time):\n",
        "  hours = int(seconds_time/3600)\n",
        "  rest_1 = seconds_time%3600\n",
        "  minutes = int(rest_1/60)\n",
        "  seconds = rest_1 - 60*minutes\n",
        "  #print(seconds)\n",
        "  print(\"Time: \", (hours), \"h \", (minutes), \"min \", round(seconds,2), \" s\")\n",
        "\n",
        "srt_time_doc2vec = time.time() # Time start\n",
        "for epoch in range(30):\n",
        "    model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), \n",
        "                     total_examples=len(train_tagged.values), epochs=1)\n",
        "    model_dbow.alpha -= 0.002\n",
        "    model_dbow.min_alpha = model_dbow.alpha\n",
        "\n",
        "seconds_doc2vec = (time.time() - srt_time_doc2vec) # Time end\n",
        "seconds_transform(seconds_doc2vec)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 67/67 [00:00<00:00, 149637.04it/s]\n",
            "100%|██████████| 67/67 [00:00<00:00, 266620.84it/s]\n",
            "100%|██████████| 67/67 [00:00<00:00, 22018.21it/s]\n",
            "100%|██████████| 67/67 [00:00<00:00, 238151.16it/s]\n",
            "100%|██████████| 67/67 [00:00<00:00, 92167.39it/s]\n",
            "100%|██████████| 67/67 [00:00<00:00, 167972.72it/s]\n",
            "100%|██████████| 67/67 [00:00<00:00, 287928.66it/s]\n",
            "100%|██████████| 67/67 [00:00<00:00, 209558.81it/s]\n",
            "100%|██████████| 67/67 [00:00<00:00, 325629.63it/s]\n",
            "100%|██████████| 67/67 [00:00<00:00, 91536.93it/s]\n",
            "100%|██████████| 67/67 [00:00<00:00, 160307.11it/s]\n",
            "100%|██████████| 67/67 [00:00<00:00, 31203.46it/s]\n",
            "100%|██████████| 67/67 [00:00<00:00, 171771.62it/s]\n",
            "100%|██████████| 67/67 [00:00<00:00, 39906.04it/s]\n",
            "100%|██████████| 67/67 [00:00<00:00, 32821.58it/s]\n",
            "100%|██████████| 67/67 [00:00<00:00, 123253.67it/s]\n",
            "100%|██████████| 67/67 [00:00<00:00, 23028.63it/s]\n",
            "100%|██████████| 67/67 [00:00<00:00, 31252.04it/s]\n",
            "100%|██████████| 67/67 [00:00<00:00, 118124.58it/s]\n",
            "100%|██████████| 67/67 [00:00<00:00, 302495.55it/s]\n",
            "100%|██████████| 67/67 [00:00<00:00, 122930.17it/s]\n",
            "100%|██████████| 67/67 [00:00<00:00, 203489.04it/s]\n",
            "100%|██████████| 67/67 [00:00<00:00, 88093.53it/s]\n",
            "100%|██████████| 67/67 [00:00<00:00, 105646.00it/s]\n",
            "100%|██████████| 67/67 [00:00<00:00, 210658.45it/s]\n",
            "100%|██████████| 67/67 [00:00<00:00, 23015.43it/s]\n",
            "100%|██████████| 67/67 [00:00<00:00, 214354.21it/s]\n",
            "100%|██████████| 67/67 [00:00<00:00, 292727.47it/s]\n",
            "100%|██████████| 67/67 [00:00<00:00, 98499.25it/s]\n",
            "100%|██████████| 67/67 [00:00<00:00, 19103.90it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Time:  0 h  0 min  1.9  s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7u6pqkbv_kXO"
      },
      "source": [
        "# Construindo a função para versão final de vetorização da feature\n",
        "def vec_for_learning(model, tagged_docs):\n",
        "  sents = tagged_docs.values\n",
        "  targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
        "  return targets, regressors\n",
        "\n",
        "y_train, x_train = vec_for_learning(model_dbow, train_tagged)\n",
        "y_test, x_test = vec_for_learning(model_dbow, test_tagged)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SoE5c6rAeA7"
      },
      "source": [
        "#print([x for x in tqdm(tagged_data)])\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB, GaussianNB # Naive Bayes\n",
        "from sklearn.tree import DecisionTreeClassifier # DTC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "#n_cv = int(7) # LR\n",
        "lr = LogisticRegression()\n",
        "lr.fit(x_train, y_train)\n",
        "y_pred_lr = lr.predict(x_test)\n",
        "# Naive Bayes\n",
        "nb = GaussianNB()\n",
        "nb.fit(x_train, y_train)\n",
        "y_pred_nb = nb.predict(x_test)\n",
        "# Decision Tree Classifier\n",
        "dt = DecisionTreeClassifier()\n",
        "dt.fit(x_train, y_train)\n",
        "y_pred_dt = dt.predict(x_test)\n",
        "#accur_lr = cross_val_score(lr, x_train, y_train, cv=n_cv, scoring='accuracy')\n",
        "#precision_lr = cross_val_score(lr, x_train, y_train, cv=n_cv, scoring='precision')\n",
        "#recall_lr = cross_val_score(lr, x_train, y_train, cv=n_cv, scoring='recall')\n",
        "#precision_lr = cross_val_score(lr, x_train, y_train, cv=n_cv, scoring='f1')\n",
        "#print(\"Doc2Vec - Acurácia LR: \", round(accur_lr.mean(),2)) # Acurácia esperada no modelo\n",
        "#print(\"Doc2Vec - Precision LR: \", round(precision_lr.mean(),2)) # Precision esperada no modelo\n",
        "#print(\"Doc2Vec - Recall LR: \", round(precision_lr.mean(),2)) # Recall esperada no modelo\n",
        "#print(\"Doc2Vec - F1 Score LR: \", round(precision_lr.mean(),2)) # F1 Score esperada no modelo\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ap6wr4C4eixy",
        "outputId": "f24edb31-ea26-422a-8732-175f07908166",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        }
      },
      "source": [
        "# LR\n",
        "precision_lr = precision_score(y_test, y_pred_lr, pos_label='1', average='binary')\n",
        "recall_lr = recall_score(y_test, y_pred_lr, pos_label='1', average='binary')\n",
        "f1_lr = f1_score(y_test, y_pred_lr, pos_label='1', average='binary')\n",
        "\n",
        "# NB\n",
        "precision_nb = precision_score(y_test, y_pred_nb, pos_label='1', average='binary')\n",
        "recall_nb = recall_score(y_test, y_pred_nb, pos_label='1', average='binary')\n",
        "f1_nb = f1_score(y_test, y_pred_nb, pos_label='1', average='binary')\n",
        "\n",
        "print(\"Accuracy LR:....... \", round(lr.score(x_test, y_test).mean(),2)) # Pode ser também accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy NB:....... \", round(nb.score(x_test, y_test).mean(),2)) # Pode ser também accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy DT:....... \", round(dt.score(x_test, y_test).mean(),2)) # Pode ser também accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy CV LR:.... 0.67\")\n",
        "print(\"Accuracy TF-IDF LR: 0.73\")\n",
        "print(\"Accuracy CV NB:.... 0.7\")\n",
        "print(\"Accuracy TF-IDF NB: 0.73\")\n",
        "print(\"Accuracy CV DT:.... 0.67\")\n",
        "print(\"Accuracy TF-IDF DT: 0.67\")\n",
        "print(\"\")\n",
        "print(\"Precision LR:....... \", round(precision_lr.mean(),2))\n",
        "print(\"Precision NB:....... \", round(precision_lr.mean(),2))\n",
        "print(\"Precision DT:....... \", round(precision_lr.mean(),2))\n",
        "print(\"Precision CV LR:.... 0.5\")\n",
        "print(\"Precision TF-IDF LR: 0.56\")\n",
        "print(\"Precision CV NB:.... 0.53\")\n",
        "print(\"Precision TF-IDF NB: 0.56\")\n",
        "print(\"Precision CV DT:.... 0.5\")\n",
        "print(\"Precision TF-IDF DT: 0.5\")\n",
        "print(\"\")\n",
        "print(\"Recall LR:....... \", round(recall_lr.mean(),2))\n",
        "print(\"Recall NB:....... \", round(recall_lr.mean(),2))\n",
        "print(\"Recall DT:....... \", round(recall_lr.mean(),2))\n",
        "print(\"Recall CV LR:.... 0.5\")\n",
        "print(\"Recall TF-IDF LR: 0.9\")\n",
        "print(\"Recall CV NB:.... 0.8\")\n",
        "print(\"Recall TF-IDF NB: 1.0\")\n",
        "print(\"Recall CV DT:.... 0.7\")\n",
        "print(\"Recall TF-IDF DT: 0.6\")\n",
        "print(\"\")\n",
        "print(\"F1 Score LR:....... \", round(f1_lr.mean(),2))\n",
        "print(\"F1 Score NB:....... \", round(f1_lr.mean(),2))\n",
        "print(\"F1 Score DT:....... \", round(f1_lr.mean(),2))\n",
        "print(\"F1 Score CV LR:.... 0.5\")\n",
        "print(\"F1 Score TF-IDF LR: 0.69\")\n",
        "print(\"F1 Score CV NB:.... 0.5\")\n",
        "print(\"F1 Score TF-IDF NB: 0.64\")\n",
        "print(\"F1 Score CV DT:.... 0.58\")\n",
        "print(\"F1 Score TF-IDF DT: 0.55\")\n",
        "#print(\"Testing AS: \", round(accuracy_score(y_test, y_pred),2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy LR:.......  0.48\n",
            "Accuracy NB:.......  0.36\n",
            "Accuracy DT:.......  0.58\n",
            "Accuracy CV LR:.... 0.67\n",
            "Accuracy TF-IDF LR: 0.73\n",
            "Accuracy CV NB:.... 0.7\n",
            "Accuracy TF-IDF NB: 0.73\n",
            "Accuracy CV DT:.... 0.67\n",
            "Accuracy TF-IDF DT: 0.67\n",
            "\n",
            "Precision LR:.......  0.42\n",
            "Precision NB:.......  0.42\n",
            "Precision DT:.......  0.42\n",
            "Precision CV LR:.... 0.5\n",
            "Precision TF-IDF LR: 0.56\n",
            "Precision CV NB:.... 0.53\n",
            "Precision TF-IDF NB: 0.56\n",
            "Precision CV DT:.... 0.5\n",
            "Precision TF-IDF DT: 0.5\n",
            "\n",
            "Recall LR:.......  0.33\n",
            "Recall NB:.......  0.33\n",
            "Recall DT:.......  0.33\n",
            "Recall CV LR:.... 0.5\n",
            "Recall TF-IDF LR: 0.9\n",
            "Recall CV NB:.... 0.8\n",
            "Recall TF-IDF NB: 1.0\n",
            "Recall CV DT:.... 0.7\n",
            "Recall TF-IDF DT: 0.6\n",
            "\n",
            "F1 Score LR:.......  0.37\n",
            "F1 Score NB:.......  0.37\n",
            "F1 Score DT:.......  0.37\n",
            "F1 Score CV LR:.... 0.5\n",
            "F1 Score TF-IDF LR: 0.69\n",
            "F1 Score CV NB:.... 0.5\n",
            "F1 Score TF-IDF NB: 0.64\n",
            "F1 Score CV DT:.... 0.58\n",
            "F1 Score TF-IDF DT: 0.55\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiZbNhx3mDAF"
      },
      "source": [
        "**2 - b)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeKRMCIQmHuP"
      },
      "source": [
        "# 4a lista - 2a questão - letra a)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}